# Exercise 16.2 - Detailed Steps

1.HAProxy 노드 접속

```
ssh -i lfs458.pem <HAproxy node ip>
```

##

2.해당 명령으로 hostname 변경

```
sudo -i
sudo hostnamectl set-hostname haproxy
sudo -i
```

##

3.haproxy 설치

```
sudo apt-get update && sudo apt-get install -y haproxy
```

##

4./etc/haproxy/haproxy.cfg 파일 수정

```
vi /etc/haproxy/haproxy.cfg
```

23번 라인 근처를 아래처럼 수정

```
defaults
	log	global
	mode	tcp
	option	tcplog
```

아래 라인 추가 (CP IP에는 본인의 CP IP입력)

```
frontend proxynode
   bind *:80
   bind *:6443
   stats uri /proxystats
   default_backend k8sServers

backend k8sServers
   balance roundrobin
   server cp1 <CP IP>:6443 check  

listen stats
     bind :9999
     mode http
     stats enable
     stats hide-version
     stats uri /stats
```

##

5.HAproxy 데몬 재시작

```
sudo systemctl restart haproxy
sudo systemctl status haproxy --no-pager
```

##

6.CP노드 터미널에서 /etc/hosts 파일 수정

```
sudo vi /etc/hosts
```

k8scp의 ip주소를 haproxy ip로 수정

```
<haproxy ip> k8scp
```

##

7.worker 노드 터미널에서 /etc/hosts 파일 수정

```
sudo vi /etc/hosts
```

k8scp의 ip주소를 haproxy ip로 수정

```
<haproxy ip> k8scp
```

##

8.웹브라우저에서 아래 주소로 접속

```
<haproxy ip>:9999/stats
```

ip 확인 명령어

```
curl ifconfig.io
```

9.CP 노드 터미널에서 API 호출

```
kubectl get node
kubectl get pod -A
```

##

10.HAproxy 통계페이지에서 새로고침을 하여 트래픽 정보(Proxynode의 Byte 트래픽)가 업데이트 되는지 확인

##

11.새로운 터미널을 실행하여,  CP2 노드 접속

```
ssh -i LFS458.pem <CP2 node ip>
```

##

12.해당 명령으로 hostname 변경

```
sudo -i
sudo hostnamectl set-hostname k8scp2
sudo -i
```

##

13. &#x20;EX3.1 과정을 참고하여, cp구성과 동일하게 사전설치 (container runtime + kubernetes 설치)



14./etc/hosts 파일 업데이트

```
sudo echo <HAporxy IP> k8scp >> /etc/hosts
```

##

15.새로운 터미널을 실행하여,  CP3 노드 접속

```
ssh -i LFS458.pem <CP3 node ip>
```

##

16.해당 명령으로 hostname 변경

```
sudo -i
sudo hostnamectl set-hostname k8scp3
sudo -i
```





17. &#x20;EX3.1 과정을 참고하여, cp구성과 동일하게 사전설치 (container runtime + kubernetes 설치)

##

18.CP노드 터미널에서 토큰생성

```
sudo kubeadm token create
```

##

19.SSL hash 생성

```
openssl x509 -pubkey \
 -in /etc/kubernetes/pki/ca.crt | openssl rsa \
 -pubin -outform der 2>/dev/null | openssl dgst \
 -sha256 -hex | sed 's/ˆ.* //'
```

##

20.새로운 CP노드 인증서 생성

```
sudo kubeadm init phase upload-certs --upload-certs
```

##

21.새로운 CP2,3를 조인하기위한조인명령어 조합

```
sudo kubeadm join k8scp:6443 --control-plane \
--token <18과정의 token> \
--discovery-token-ca-cert-hash sha256:<19과정의 ssl hash> \
--certificate-key <20과정의 certs>
```

##

22. &#x20;21에서 조합한 명령어를 cp2 노드와 cp3노드에서 실행

(Join 시 오류사항 트러블슈팅)

error execution phase preflight: \[preflight] Some fatal errors occurred:

\[ERROR FileContent--proc-sys-net-ipv4-ip\_forward]: /proc/sys/net/ipv4/ip\_forward contents are not set to \~

아래명령어 입력후 재조인

```
echo '1' > /proc/sys/net/ipv4/ip_forward
```

(Join 시 오류사항 트러블슈팅 2)

error execution phase preflight: \[preflight] Some fatal errors occurred:

\[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist \[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`

아래 명령어 입력후 재조인

```
modprobe br_netfilter

echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables
```







> 23\~25 단계는 cp2, cp3 노드에서 모두 실행합니다.

23.ubuntu 사용자로 전환

```
exit
```

##

24.kubeconfig 파일 생성

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

##

25.명령어 자동완성 기능 추가

```
echo 'source <(kubectl completion bash)' >>~/.bashrc
source <(kubectl completion bash)
```

##

26.HAProxy 노드로 이동하여 설정파일 수정

```
vi /etc/haproxy/haproxy.cfg
```

```
backend k8sServers
   balance roundrobin
   server cp1 <CP1 ip>:6443 check
   server cp2 <CP2 ip>:6443 check
   server cp3 <CP3 ip>:6443 check
```

##

27.HAproxy 데몬 재시작

```
sudo systemctl restart haproxy
sudo systemctl status haproxy --no-pager
```

##

28.웹브라우저에서 :9999/stats 로 이동하여 추가한 노드들의 연결상태 확인

##

29.CP노드에서 Node 목록확인

```
kubectl get nodes
```

##

30.kubectl 명령어를 사용하여 api호출을 실행하고 haproxy 통계사이트에서 로드 분산 여부 확인

```
for i in {1..10}; do kubectl get pod; done
```

31.실행중인 etcd Pod 확인

```
kubectl get pod -n kube-system -l component=etcd
```

32.ETCD Pod 로그 확인

```
kubectl -n kube-system logs etcd-k8scp2 | grep leader -A 5 -B 5
```

33.실행중인 ETCD Pod의 IP 주소 확인

```
kubectl get pod -n kube-system -l component=etcd -o wide 
```

34.ETCD 클러스터 상태 확인

```
kubectl -n kube-system exec -it etcd-cp -- \
etcdctl -w table \
--endpoints \
$(kubectl get pod etcd-cp -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp2 -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp3 -n kube-system -o=jsonpath='{.status.podIP}'):2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key \
endpoint status
```

35.CP노드에서 컨테이너 정지

```
sudo systemctl stop kubelet
sudo crictl stop $(sudo crictl ps -q)
sudo systemctl stop containerd
sudo systemctl start kubelet
```

36.ETCD Pod 로그 확인

```
kubectl -n kube-system logs etcd-k8scp2 | grep leader -A 5 -B 5
```

37.HAProxy 웹브라우저 대시보드에서 새로고침하여 노드 연결상태 확인

38.다른CP 노드에서ETCD 클러스터 상태 확인 - 리더가 변경된 것을 확인

```
kubectl -n kube-system exec -it etcd-k8scp2 -- \
etcdctl -w table \
--endpoints \
$(kubectl get pod etcd-cp -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp2 -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp3 -n kube-system -o=jsonpath='{.status.podIP}'):2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key \
endpoint status
```

39.NODE 상태확인

```
kubectl get nodes
```

CP노드에서 컨테이너런타임 재실행

```
sudo systemctl start containerd
```

40.모든 Pod가 정상적으로 실행되고 있는지 확인

```
kubectl get pod -A
```

41.ETCD 클러스터 상태 확인

```
kubectl -n kube-system exec -it etcd-k8scp2 -- \
etcdctl -w table \
--endpoints \
$(kubectl get pod etcd-cp -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp2 -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp3 -n kube-system -o=jsonpath='{.status.podIP}'):2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key \
endpoint status
```
