# Exercise 16.2 - Detailed Steps

1.HAProxy 노드 접속

```
ssh -i LFS458.pem <HAproxy node ip>
```

##

2.해당 명령으로 hostname 변경

```
sudo -i
sudo hostnamectl set-hostname haproxy
sudo -i
```

##

3.haproxy 설치

```
sudo apt-get update && sudo apt-get install -y haproxy
```

##

4./etc/haproxy/haproxy.cfg 파일 수정

```
vi /etc/haproxy/haproxy.cfg
```

23번 라인 근처를 아래처럼 수정

```
defaults
	log	global
	mode	tcp
	option	tcplog
```

아래 라인 추가 (CP IP에는 본인의 CP IP입력)

```
frontend proxynode
   bind *:80
   bind *:6443
   stats uri /proxystats
   default_backend k8sServers

backend k8sServers
   balance roundrobin
   server cp1 <CP IP>:6443 check  

listen stats
     bind :9999
     mode http
     stats enable
     stats hide-version
     stats uri /stats
```

##

5.HAproxy 데몬 재시작

```
sudo systemctl restart haproxy
sudo systemctl status haproxy --no-pager
```

##

6.CP노드 터미널에서 /etc/hosts 파일 수정

```
sudo vi /etc/hosts
```

k8scp의 ip주소를 haproxy ip로 수정

```
<haproxy ip> k8scp
```

##

7.worker 노드 터미널에서 /etc/hosts 파일 수정

```
sudo vi /etc/hosts
```

k8scp의 ip주소를 haproxy ip로 수정

```
<haproxy ip> k8scp
```

##

8.웹브라우저에서 아래 주소로 접속

```
<haproxy ip>:9999/stats
```

ip 확인 명령어

```
curl ifconfig.io
```



9.CP 노드 터미널에서 API 호출

```
kubectl get node
kubectl get pod -A
```

##

10.HAproxy 통계페이지에서 새로고침을 하여 트래픽 정보(Proxynode의 Byte 트래픽)가 업데이트 되는지 확인

##

11.CP2 노드 접속

```
ssh -i LFS458.pem <CP2 node ip>
```

##

12.해당 명령으로 hostname 변경

```
sudo -i
sudo hostnamectl set-hostname k8scp2
sudo -i
```

##

13.이전 cp구성과 동일하게 사전설치 (container runtime + kubernetes 설치)

```
apt update && apt install curl apt-transport-https vim git wget gnupg2 software-properties-common apt-transport-https ca-certificates uidmap -y
```

```
swapoff -a
```

```
modprobe overlay
modprobe br_netfilter
```

```
cat << EOF | tee /etc/sysctl.d/kubernetes.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

cat <<EOF> /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
```

```
 echo "deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.24/xUbuntu_18.04/ /" | sudo tee -a /etc/apt/sources.list.d/cri-0.list
 curl -L http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/1.24/xUbuntu_18.04/Release.key | sudo apt-key add -
 echo "deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_18.04/ /" | sudo tee -a /etc/apt/sources.list.d/libcontainers.list
```

```
apt update && apt install -y cri-o cri-o-runc
```

```
rm -rf /etc/cni/net.d
```

```
systemctl daemon-reload
systemctl enable crio
systemctl restart crio
```

```
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
```

```
curl -s \
https://packages.cloud.google.com/apt/doc/apt-key.gpg \ | apt-key add -
```

```
apt-get update
```

```
apt-get install -y kubeadm=1.25.1-00 kubelet=1.25.1-00 kubectl=1.25.1-00
```

```
apt-mark hold kubelet kubeadm kubectl
```

##

14./etc/hosts 파일 업데이트

```
sudo echo <HAporxy IP> k8scp >> /etc/hosts
```

##

15.CP3 노드 접속

```
ssh -i LFS458.pem <CP3 node ip>
```

##

16.해당 명령으로 hostname 변경

```
sudo -i
sudo hostnamectl set-hostname k8scp3
sudo -i
```

17.13\~14단계 진행

##

18.CP노드 터미널에서 토큰생성

```
sudo kubeadm token create
```

##

19.SSL hash 생성

```
openssl x509 -pubkey \
 -in /etc/kubernetes/pki/ca.crt | openssl rsa \
 -pubin -outform der 2>/dev/null | openssl dgst \
 -sha256 -hex | sed 's/ˆ.* //'
```

##

20.새로운 CP노드 인증서 생성

```
sudo kubeadm init phase upload-certs --upload-certs
```

##

21.새로운 CP2,3 조인명령어 조합

```
sudo kubeadm join k8scp:6443 --control-plane \
--token <18과정의 token> \
--discovery-token-ca-cert-hash sha256:<19과정의 ssl hash> \
--certificate-key <20과정의 certs>
```

##

22.21에서 조합한 명령어를 cp2 노드와 cp3노드에서 실행



(Join 시 오류사항 트러블슈팅)

error execution phase preflight: \[preflight] Some fatal errors occurred:

&#x20;   \[ERROR FileContent--proc-sys-net-ipv4-ip\_forward]: /proc/sys/net/ipv4/ip\_forward contents are not set to \~

아래명령어 입력후 재조인

```
echo '1' > /proc/sys/net/ipv4/ip_forward
```

(Join 시 오류사항 트러블슈팅 2)

error execution phase preflight: \[preflight] Some fatal errors occurred:&#x20;

\[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist \[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`

아래 명령어 입력후 재조인

```
modprobe br_netfilter

echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables
```





> 23\~25 단계는 cp2, cp3 노드에서 모두 실행합니다.

23.ubuntu 사용자로 전환

```
exit
```

##

24.kubeconfig 파일 생성

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

##

25.명령어 자동완성 기능 추가

```
echo 'source <(kubectl completion bash)' >>~/.bashrc
source <(kubectl completion bash)
```

##

26.HAProxy 노드로 이동하여 설정파일 수정

```
vi /etc/haproxy/haproxy.cfg
```

```
backend k8sServers
   balance roundrobin
   server cp1 <CP1 ip>:6443 check
   server cp2 <CP2 ip>:6443 check
   server cp3 <CP3 ip>:6443 check
```

##

1. HAproxy 데몬 재시작

```
sudo systemctl restart haproxy
sudo systemctl status haproxy --no-pager
```

##

28.웹브라우저에서 :9999/stats 로 이동하여 추가한 노드들의 연결상태 확인

##

29.CP노드에서 Node 목록확인

```
kubectl get nodes
```

##

30.kubectl 명령어를 사용하여 api호출을 실행하고 haproxy 통계사이트에서 로드 분산 여부 확인

```
for i in {1..10}; do kubectl get pod; done
```



31. 실행중인 etcd Pod 확인

<pre><code><strong>kubectl get pod -n kube-system -l component=etcd
</strong></code></pre>



32. ETCD Pod 로그 확인

```
kubectl -n kube-system logs etcd-k8scp2 | grep leader -A 5 -B 5
```



33. &#x20; 실행중인 ETCD Pod의 IP 주소 확인&#x20;

```
kubectl get pod -n kube-system -l component=etcd -o wide 
```



34. &#x20;ETCD 클러스터 상태 확인

```
kubectl -n kube-system exec -it etcd-cp -- \
etcdctl -w table \
--endpoints \
$(kubectl get pod etcd-cp -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp2 -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp3 -n kube-system -o=jsonpath='{.status.podIP}'):2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key \
endpoint status
```



35. CP노드에서 컨테이너 정지

```
sudo systemctl stop kubelet
sudo crictl stop $(sudo crictl ps -q)
sudo systemctl stop crio.service
sudo systemctl start kubelet
```



36. &#x20;ETCD Pod 로그 확인

```
kubectl -n kube-system logs etcd-k8scp2 | grep leader -A 5 -B 5
```



37. HAProxy 웹브라우저 대시보드에서 새로고침하여 노드 연결상태 확인



38. &#x20;CP 노드에서ETCD 클러스터 상태 확인 - 리더가 변경된 것을 확인

```
kubectl -n kube-system exec -it etcd-k8scp2 -- \
etcdctl -w table \
--endpoints \
$(kubectl get pod etcd-cp -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp2 -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp3 -n kube-system -o=jsonpath='{.status.podIP}'):2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key \
endpoint status
```



39. &#x20;NODE 상태확인

```
kubectl get nodes
```



40. 모든 Pod가 정상적으로 실행되고 있는지 확인

```
kubectl get pod -A
```



41. ETCD 클러스터 상태 확인

```
kubectl -n kube-system exec -it etcd-k8scp2 -- \
etcdctl -w table \
--endpoints \
$(kubectl get pod etcd-cp -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp2 -n kube-system -o=jsonpath='{.status.podIP}'):2379,\
$(kubectl get pod etcd-k8scp3 -n kube-system -o=jsonpath='{.status.podIP}'):2379 \
--cacert /etc/kubernetes/pki/etcd/ca.crt \
--cert /etc/kubernetes/pki/etcd/server.crt \
--key /etc/kubernetes/pki/etcd/server.key \
endpoint status
```
